RAG Chat System
Docker-based RAG (Retrieval-Augmented Generation) system with parent-child chunking, local embeddings, Zotero integration, and API-based LLM integration.
Features

Multi-format document upload (PDF, DOCX, TXT, MD)
Zotero Library Integration (automatic paper sync)
Multiple persistent chats
RAG-based question answering with reranking
Parent-child chunking strategy
Web-based interface

Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Browser                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Frontend  â”‚
                  â”‚  (Nginx)    â”‚
                  â”‚  Port 80    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ API Calls
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Backend   â”‚
                  â”‚  (FastAPI)  â”‚
                  â”‚  Port 8000  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
        â–¼                â–¼                â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Qdrant  â”‚    â”‚ LLM API  â”‚    â”‚  Zotero  â”‚
 â”‚Port 6333 â”‚    â”‚ (Remote) â”‚    â”‚   API    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
RAG System Workflow
Phase 1: Indexierung (Dokument-Upload)
ğŸ“„ PDF Upload / Zotero Sync
    â†“
ğŸ”„ Docling Converter
    â€¢ PDF â†’ Strukturiertes Markdown
    â€¢ Fallback: PyPDF (bei libGL-Fehler)
    â†“
ğŸ“‹ Metadata Extraction
    â€¢ Titel, Autor, Seitenzahl
    â€¢ Optional: LLM-basierte Analyse
    â†“
âœ‚ï¸ Document Chunking (Parent-Child)
    â€¢ Parent: 2000 Tokens (Kontext)
    â€¢ Child: 400 Tokens (Retrieval)
    â€¢ Overlap: Konsistenz
    â†“
ğŸ”¢ Embedding Generation
    â€¢ Batch-Processing (32 Chunks/Batch)
    â€¢ Model: mxbai-embed-large-v1
    â€¢ Output: 1024-dim Vektoren
    â†“
ğŸ’¾ Qdrant Vector Storage
    â€¢ Collection: doc_X
    â€¢ Scalar Quantization (Kompression)
    â€¢ Hybrid Search (Dense + Metadata)
Phase 2: Retrieval (User-Query)
â“ User Query
    â†“
ğŸ”¢ Query Embedding
    â€¢ Gleicher Encoder wie Dokumente
    â†“
ğŸ” Vector Search (Qdrant)
    â€¢ Top-K: 20 Kandidaten
    â€¢ Cosine Similarity
    â†“
ğŸ¯ Reranking
    â€¢ Model: bge-reranker-v2-m3
    â€¢ PrÃ¤zise Query-Chunk-Bewertung
    â€¢ Top-K: 6 beste Chunks
    â†“
ğŸ“š Context Assembly
    â€¢ Parent-Chunks laden (mehr Kontext)
    â€¢ Optional: Neighbor Expansion (Â±4 Chunks)
    â†“
ğŸ¤– LLM Generation
    â€¢ Provider: Claude/OpenAI/Ollama
    â€¢ Prompt: Query + Context
    â€¢ Stream Response
    â†“
âœ… Antwort an User
Prerequisites

Docker and Docker Compose
LLM API key (OpenAI, Anthropic, or Ollama)
Optional: Zotero account with API key

Setup
1. Clone Repository
bashgit clone https://github.com/DuncanSARapp/SMA-Abgabe.git
cd SMA-Abgabe
2. Configure Environment
bashcp .env.example .env
Minimal Configuration (.env):
env# LLM Provider (required)
ANTHROPIC_API_KEY=sk-ant-...
# oder
OPENAI_API_KEY=sk-...

# Zotero (optional)
ZOTERO_LIBRARY_ID=your-library-id
ZOTERO_API_KEY=your-zotero-key
ZOTERO_LIBRARY_TYPE=user  # oder "group"
Erweiterte Konfiguration (optional):
env# Models (defaults in settings.py)
EMBEDDING_MODEL=mixedbread-ai/mxbai-embed-large-v1
RERANKER_MODEL=BAAI/bge-reranker-v2-m3

# LLM Settings
LLM_PROVIDER=anthropic  # anthropic, openai, ollama
LLM_MODEL=claude-sonnet-4-20250514
LLM_TEMPERATURE=0.0

# Retrieval
TOP_K_RETRIEVAL=20
TOP_K_RERANK=6

Hinweis: Alle Parameter mit Defaults in backend/config/settings.py mÃ¼ssen nicht in .env gesetzt werden.

3. Run Setup Script
bash./setup.sh
The setup.sh script validates prerequisites, creates a virtual environment, installs dependencies, validates Docker configuration, and starts all services.
4. Access Application
Frontend: http://localhost:80
API Docs: http://localhost:8000/docs